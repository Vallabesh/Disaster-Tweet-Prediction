{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b234582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "import re\n",
    "from sklearn.externals\n",
    "import joblib\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, Conv2D\n",
    "from keras.layers import MaxPooling1D, GlobalMaxPooling1D ,MaxPooling2D\n",
    "from keras.layers import Dense, Input , Dropout, Flatten\n",
    "import h5py\n",
    "from tensorflow.python.keras.callbacks import TensorBoard,ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n",
    "from time import time\n",
    "import pickle \n",
    "from numpy import zeros\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization)\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4daf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"\\'I'm\", \"I am\", phrase)\n",
    "    phrase = re.sub(r\"I\\'ve\", \"I have\", phrase)\n",
    "    phrase = re.sub(r\"isn\\'t\", \"is not\", phrase)\n",
    "    phrase = re.sub(r\"we\\'ll\", \"we will\", phrase)\n",
    "    phrase = re.sub(r\"we\\'re\", \"we are\", phrase)\n",
    "    phrase = re.sub(r\"we\\'ve\", \"we have\", phrase)\n",
    "    phrase = re.sub(r\"weren\\'t\", \"were not\", phrase)\n",
    "    phrase = re.sub(r\"quake\", \"earthquake\", phrase)\n",
    "    phrase = re.sub(r\"nado\", \"tornado\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3bfb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(sentence):\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n",
    "    sentence = decontracted(sentence)\n",
    "    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n",
    "    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n",
    "    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91732b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbe71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80bd854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preprocessed_tweets = []\n",
    "for sentence in df_train['text'].values:\n",
    "    preprocessed_tweets.append(clean_tweet(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ec4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0cdc7ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token_tweet.pkl']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_tweet = Tokenizer()\n",
    "tokenizer_tweet.fit_on_texts(preprocessed_tweets)\n",
    "joblib.dump(tokenizer_tweet, 'token_tweet.pkl')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cea672d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenlen = len(tokenizer_tweet.word_index)+1\n",
    "tweet_token_text  = tokenizer_tweet.texts_to_sequences(preprocessed_tweets)\n",
    "X_pad = pad_sequences(tweet_token_text, maxlen=35 , padding='post' )\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, Y, stratify=df_train['target'], test_size=0.20)\n",
    "embedding_matrix = zeros((tweet_tokenlen, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "672546b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = {}\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    split_line  = line.split()\n",
    "    word = split_line[0]\n",
    "    embedding  = np.asarray(split_line [1:], dtype='float32')\n",
    "    glove_model [word] = embedding \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f4737a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_keys = glove_model.keys()\n",
    "for word, i in tokenizer_tweet.word_index.items():\n",
    "    if word in glove_keys:\n",
    "        embedding_vector = glove_model.get(word)\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "19599675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15615, 100)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "78d682bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53cb1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08806896",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weightsflask1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy',save_best_only=True, mode='max', verbose=1)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/model_flask\".format(time()))\n",
    "callbacks_1 = [checkpoint,tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b998e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m embedding_vecor_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model7 \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[0;32m      3\u001b[0m model7\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39membedding_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], output_dim\u001b[38;5;241m=\u001b[39membedding_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], weights \u001b[38;5;241m=\u001b[39m [embedding_matrix], input_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m35\u001b[39m))\n\u001b[0;32m      4\u001b[0m model7\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m108\u001b[39m,return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model7 = Sequential()\n",
    "model7.add(Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights = [embedding_matrix], input_length=35))\n",
    "model7.add(LSTM(108,return_sequences=True))\n",
    "model7.add(Dropout(0.5))\n",
    "model7.add(LSTM(64,return_sequences=True))\n",
    "model7.add(Dropout(0.5))\n",
    "model7.add(LSTM(32))\n",
    "model7.add(Dropout(0.3))\n",
    "model7.add(Dense(1, activation='relu'))\n",
    "print(model7.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b83b1d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.7379\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.72095, saving model to weightsflask1.hdf5\n",
      "96/96 [==============================] - 27s 210ms/step - loss: 0.6827 - accuracy: 0.7379 - val_loss: 0.6629 - val_accuracy: 0.7209\n",
      "Epoch 2/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7732\n",
      "Epoch 00002: val_accuracy improved from 0.72095 to 0.80368, saving model to weightsflask1.hdf5\n",
      "96/96 [==============================] - 19s 196ms/step - loss: 0.5653 - accuracy: 0.7732 - val_loss: 0.4940 - val_accuracy: 0.8037\n",
      "Epoch 3/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.5099 - accuracy: 0.8153\n",
      "Epoch 00003: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 19s 194ms/step - loss: 0.5099 - accuracy: 0.8153 - val_loss: 0.9121 - val_accuracy: 0.7912\n",
      "Epoch 4/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.8276\n",
      "Epoch 00004: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 19s 196ms/step - loss: 0.5582 - accuracy: 0.8276 - val_loss: 0.5388 - val_accuracy: 0.7945\n",
      "Epoch 5/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4893 - accuracy: 0.8294\n",
      "Epoch 00005: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 18s 193ms/step - loss: 0.4893 - accuracy: 0.8294 - val_loss: 0.5295 - val_accuracy: 0.7617\n",
      "Epoch 6/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4648 - accuracy: 0.8527\n",
      "Epoch 00006: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 18s 192ms/step - loss: 0.4648 - accuracy: 0.8527 - val_loss: 1.2116 - val_accuracy: 0.7466\n",
      "Epoch 7/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4526 - accuracy: 0.8660\n",
      "Epoch 00007: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 18s 192ms/step - loss: 0.4526 - accuracy: 0.8660 - val_loss: 0.7329 - val_accuracy: 0.8024\n",
      "Epoch 8/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.8782\n",
      "Epoch 00008: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 19s 193ms/step - loss: 0.4144 - accuracy: 0.8782 - val_loss: 0.6640 - val_accuracy: 0.7643\n",
      "Epoch 9/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.8545\n",
      "Epoch 00009: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 19s 193ms/step - loss: 0.4513 - accuracy: 0.8545 - val_loss: 1.1121 - val_accuracy: 0.7656\n",
      "Epoch 10/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.8629\n",
      "Epoch 00010: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 19s 193ms/step - loss: 0.3622 - accuracy: 0.8629 - val_loss: 0.7716 - val_accuracy: 0.7774\n",
      "Epoch 11/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8970\n",
      "Epoch 00011: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 18s 192ms/step - loss: 0.3667 - accuracy: 0.8970 - val_loss: 1.7699 - val_accuracy: 0.8004\n",
      "Epoch 12/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.9000\n",
      "Epoch 00012: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 18s 192ms/step - loss: 0.3776 - accuracy: 0.9000 - val_loss: 1.1166 - val_accuracy: 0.7676\n",
      "Epoch 13/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4142 - accuracy: 0.9232\n",
      "Epoch 00013: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 18s 193ms/step - loss: 0.4142 - accuracy: 0.9232 - val_loss: 2.1299 - val_accuracy: 0.7347\n",
      "Epoch 14/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.9278\n",
      "Epoch 00014: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 18s 192ms/step - loss: 0.4271 - accuracy: 0.9278 - val_loss: 1.8982 - val_accuracy: 0.7886\n",
      "Epoch 15/15\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3317 - accuracy: 0.9355\n",
      "Epoch 00015: val_accuracy did not improve from 0.80368\n",
      "96/96 [==============================] - 18s 193ms/step - loss: 0.3317 - accuracy: 0.9355 - val_loss: 1.8445 - val_accuracy: 0.7951\n"
     ]
    }
   ],
   "source": [
    "model7.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "history_7 = model7.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=64,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=15,callbacks=[callbacks_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9d838da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_66_layer_call_fn, lstm_cell_66_layer_call_and_return_conditional_losses, lstm_cell_67_layer_call_fn, lstm_cell_67_layer_call_and_return_conditional_losses, lstm_cell_68_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_flask\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_flask\\assets\n"
     ]
    }
   ],
   "source": [
    "model7.save('model_flask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c05cbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eeb389",
   "metadata": {},
   "source": [
    "## Here is where I am unable to predict properly, even thought my function and process is correct from previous Direct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(string):\n",
    "    model_flask = keras.models.load_model('model_flask')\n",
    "    tokenizer_tweet = joblib.load('token_tweet.pkl')\n",
    "    tweet_tokenlen = len(tokenizer_tweet.word_index)+1\n",
    "    tweet = clean_tweet(string)\n",
    "    tweet_token_text  = tokenizer_tweet.texts_to_sequences(tweet)\n",
    "    tweet_token_text = pad_sequences(tweet_token_text, maxlen=35 , padding='post' )\n",
    "    #embedding_matrix = np.load('embeddings.npy')\n",
    "    pred = model_flask.predict_classes(tweet_token_text)\n",
    "    #pred =np.argmax(pred)\n",
    "\n",
    "    print(pred)\n",
    "    if pred == 0:\n",
    "        prediction = \"Hoax\"\n",
    "    else:\n",
    "        prediction = \"Real Disaster\"\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60bb0d4",
   "metadata": {},
   "source": [
    "## Here I get the no attribute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "141841dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_tweet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mForest fire near La Ronge Sask. Canada\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m, in \u001b[0;36mpredict_tweet\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      7\u001b[0m tweet_token_text \u001b[38;5;241m=\u001b[39m pad_sequences(tweet_token_text, maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m35\u001b[39m , padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#embedding_matrix = np.load('embeddings.npy')\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_flask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_classes\u001b[49m(tweet_token_text)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#pred =np.argmax(pred)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "print(predict_tweet('Forest fire near La Ronge Sask. Canada'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e709e",
   "metadata": {},
   "source": [
    "## I go through stack over flow and find this solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e10b2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(string):\n",
    "    model_flask = keras.models.load_model('model_flask')\n",
    "    tokenizer_tweet = joblib.load('token_tweet.pkl')\n",
    "    tweet = clean_tweet(string)\n",
    "    #print(tweet)\n",
    "    tweet=  [tweet]\n",
    "    tweet_token_text  = tokenizer_tweet.texts_to_sequences(tweet)\n",
    "    #print(tweet_token_text)\n",
    "    #print(len(tweet_token_text))\n",
    "    tweet_token_text = pad_sequences(tweet_token_text, maxlen=35 , padding='post')\n",
    "    #print(tweet_token_text.shape)\n",
    "    #embedding_matrix = np.load('embeddings.npy')\n",
    "    pred = model_flask.predict(tweet_token_text)\n",
    "    #pred =np.argmax(pred,axis=1)\n",
    "    #predictions = (model_flask.predict(tweet_token_text) > 0.5).astype(\"int32\")\n",
    "    pred = np.array(list(map(lambda x : 'Real Disaster' if x > 0.5 else 'Hoax',pred)))\n",
    "    #pred = np.array(list(map(lambda x : 'positive' if x > 0.5 else 'negative',pred)))\n",
    "    print(pred)\n",
    "    print(pred.shape)\n",
    "    print(type(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99c4ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(string):\n",
    "    model_flask = keras.models.load_model('model_flask')\n",
    "    tokenizer_tweet = joblib.load('token_tweet.pkl')\n",
    "    tweet = clean_tweet(string)\n",
    "    tweet=  [tweet]\n",
    "    tweet_token_text  = tokenizer_tweet.texts_to_sequences(tweet)\n",
    "    tweet_token_text = pad_sequences(tweet_token_text, maxlen=35 , padding='post')\n",
    "    #pred = model_flask.predict(tweet_token_text)\n",
    "    pred = (model_flask.predict(tweet_token_text) > 0.5).astype(\"int32\")\n",
    "    if pred == 0:\n",
    "        return \"It's a Hoax\"\n",
    "    else:\n",
    "        return \"Real disaster!!\"\n",
    "    #pred = (list(map(lambda x : 'Real Disaster' if x > 0.5 else 'Hoax',pred)))\n",
    "    #return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "93e0d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Forest fire near La Ronge Sask. Canada'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d6172de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d720010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=  a.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "23171d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Forest fire near La Ronge Sask. Canada']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ba46273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Real disaster!!\n"
     ]
    }
   ],
   "source": [
    "print(predict_tweet('Forest fire near La Ronge Sask. Canada'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "767c4a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "It's a Hoax\n"
     ]
    }
   ],
   "source": [
    "print(predict_tweet('Love my girlfriend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eec2f605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ablaze anaheim police arsonist deliberately set black church north carolina ablaze\n",
      "[[490, 13, 647, 6302, 159, 86, 1426, 526, 4229, 490]]\n",
      "1\n",
      "(1, 35)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "['positive']\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(predict_tweet('ablaze,Anaheim,Police: Arsonist Deliberately Set Black Church In North CarolinaåÊAblaze http://t.co/pcXarbH9An'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ca194",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.round()\n",
    "# array([0., 0., 0., 1., 1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51669cfc",
   "metadata": {},
   "source": [
    "## Again the Above error appears. Please help me solve this, I tried few more solutions from stackoverflow but none work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa2c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
